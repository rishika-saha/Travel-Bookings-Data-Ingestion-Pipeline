{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adee9835-be85-4a77-bdbd-df35bb07331a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, sum as _sum\n",
    "from delta.tables import DeltaTable\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "import os\n",
    "\n",
    "print(os.environ['SPARK_VERSION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24df1a93-f883-406e-b03a-e1d789abc315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Defining Path\n",
    "date_str = dbutils.widgets.get(\"arrival_date\")\n",
    "\n",
    "booking_data = f\"/Volumes/test_gds/default/booking_data/bookings_{date_str}.csv\"\n",
    "customer_data = f\"/Volumes/test_gds/default/customer_data/customers_{date_str}.csv\"\n",
    "print(booking_data)\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f5f5a7-0cb3-4e9a-8aea-125fc69bd9ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reading Data from the defined path\n",
    "\n",
    "booking_df = spark.read.\\\n",
    "        format(\"csv\").\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferSchema\", \"true\").\\\n",
    "        option(\"quote\",\"\\\"\").\\\n",
    "        option(\"multiline\", \"true\").\\\n",
    "        load(booking_data)\n",
    "\n",
    "customer_df = spark.read.\\\n",
    "        format(\"csv\").\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferSchema\", \"true\").\\\n",
    "        option(\"quote\",\"\\\"\").\\\n",
    "        option(\"multiline\", \"true\").\\\n",
    "        load(customer_data)\n",
    "\n",
    "print(booking_df)\n",
    "print(customer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255959e2-358b-4e9d-b4f7-b9b134e25272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Visulazing the DataFrames and Schemas\n",
    "\n",
    "booking_df.printSchema()\n",
    "customer_df.printSchema()\n",
    "booking_df.show(5)\n",
    "customer_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651adc02-e6cf-49cf-9fb0-680d757ea81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Quality checks on Booking Data and Checking Data Types\n",
    "\n",
    "booking_check = Check(spark, CheckLevel.Error, \"Booking Data Check\")\\\n",
    "        .hasSize(lambda x: x > 0)\\\n",
    "        .isUnique(\"booking_id\")\\\n",
    "        .isComplete(\"customer_id\")\\\n",
    "        .isComplete(\"amount\")\\\n",
    "        .isNonNegative(\"amount\")\\\n",
    "        .isNonNegative(\"quantity\")\\\n",
    "        .isNonNegative(\"discount\")\n",
    "\n",
    "customer_check = Check(spark, CheckLevel.Error, \"Customer Data Check\")\\\n",
    "        .hasSize(lambda x: x > 0)\\\n",
    "        .isUnique(\"customer_id\")\\\n",
    "        .isComplete(\"customer_name\")\\\n",
    "        .isComplete(\"customer_address\")\\\n",
    "        .isComplete(\"email\")\n",
    "\n",
    "\n",
    "# Run the verification suite\n",
    "booking_dq_check = VerificationSuite(spark).onData(booking_df).addCheck(booking_check).run()\n",
    "customer_dq_check = VerificationSuite(spark).onData(customer_df).addCheck(customer_check).run()\n",
    "\n",
    "booking_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, booking_dq_check)\n",
    "customer_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark,customer_dq_check)\n",
    "\n",
    "display(booking_dq_check_df)\n",
    "display(customer_dq_check_df)\n",
    "\n",
    "if booking_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Booking Data Quality Check Failed\")\n",
    "\n",
    "if customer_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Customer Data Quality Check Failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df0fd86-1b38-4f73-a930-d19f39a47d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add column in booking_data\n",
    "booking_new_df = booking_df.withColumn(\"timestamp\", current_timestamp())\n",
    "booking_new_df.show(5)\n",
    "\n",
    "# Join boking data with customer data\n",
    "df_joined = booking_new_df.join(customer_df, \"customer_id\", \"inner\")\n",
    "df_joined.show(5)\n",
    "\n",
    "# Add Filters\n",
    "df_joined_filter = df_joined.withColumn(\"TotalCost\", col(\"amount\") - col(\"discount\")).filter(col(\"quantity\") > 0)\n",
    "df_joined_filter.show(5)\n",
    "\n",
    "#Group by and aggregate\n",
    "df_joined_agg = df_joined_filter.groupBy(\"customer_id\", \"booking_type\").agg(_sum(\"TotalCost\").alias(\"TotalCost\"), _sum(\"quantity\").alias(\"TotalQuantity\"))\n",
    "\n",
    "df_joined_agg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b9b801-4a2f-4694-a039-696e32153de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_table = \"test_gds.default.booking_fact\"\n",
    "fact_table_exists = spark._jsparkSession.catalog().tableExists(fact_table)\n",
    "\n",
    "if fact_table_exists :\n",
    "    df_existing_table = spark.read.format(\"delta\").table(fact_table)\n",
    "    df_combined = df_existing_table.unionByName(df_joined_agg, allowMissingColumns=True)\n",
    "    df_final_agg = df_combined.groupBy(\"customer_id\", \"booking_type\")\\\n",
    "    .agg(_sum(\"TotalCost\").alias(\"TotalCost\"), _sum(\"TotalQuantity\").alias(\"TotalQuantity\"))\n",
    "else:\n",
    "    df_final_agg = df_joined_agg\n",
    "\n",
    "df_final_agg.show(5)\n",
    "\n",
    "#Write this data to delta table\n",
    "df_final_agg.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fact_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7fe7dc-d931-4caa-bbde-675c4b1938e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scd_dim_table = \"test_gds.default.customer_dim\"\n",
    "scd_dim_table_exists = spark._jsparkSession.catalog().tableExists(scd_dim_table)\n",
    "\n",
    "if scd_dim_table_exists :\n",
    "    scd_table = DeltaTable.forName(spark, scd_dim_table)\n",
    "    display(scd_table.toDF())\n",
    "\n",
    "    # Perform SCD2 merge logic\n",
    "    scd_table.alias(\"scd\") \\\n",
    "        .merge(\n",
    "            customer_df.alias(\"updates\"),\n",
    "            \"scd.customer_id = updates.customer_id and scd.valid_to = '9999-12-31'\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"valid_to\": \"updates.valid_from\",\n",
    "        }) \\\n",
    "        .execute()\n",
    "\n",
    "    customer_df.write.format(\"delta\").mode(\"append\").saveAsTable(scd_dim_table)\n",
    "else:\n",
    "    customer_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(scd_dim_table)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scd2_merge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
